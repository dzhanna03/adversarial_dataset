{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNuec9tbxJnQ2cfSCHKkUz+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IxSG88ITSzZo"},"outputs":[],"source":["!pip install transformers pandas scikit-learn"]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n","\n","df = pd.read_csv('dataset_core_v2.csv')\n","df['numeric_label'] = df['label'].map({\"entailment\": 1, \"non-entailment\": 0})\n","\n","def predict_with_probabilities(premise, hypothesis):\n","    inputs = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\").to(device)\n","    outputs = model(**inputs)\n","    probs = torch.softmax(outputs.logits, dim=-1)\n","    entailment_prob = probs[0, 0].item()\n","    non_entailment_prob = (probs[0, 1] + probs[0, 2]).item()\n","    predicted_label = 1 if entailment_prob > non_entailment_prob else 0\n","    return predicted_label, entailment_prob, non_entailment_prob\n","\n","predictions = []\n","for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n","    premise, hypothesis, true_label, alternation = row['premise'], row['hypothesis'], row['numeric_label'], row['alternation']\n","    pred_label, entailment_prob, non_entailment_prob = predict_with_probabilities(premise, hypothesis)\n","    predictions.append([premise, hypothesis, true_label, pred_label, entailment_prob, non_entailment_prob, alternation])\n","\n","detailed_df = pd.DataFrame(predictions, columns=['Premise', 'Hypothesis', 'True Label', 'Predicted Label', 'Entailment Prob', 'Non-Entailment Prob', 'Alternation'])\n","detailed_df.to_csv('predictions_DeBERTa_base.csv', index=False)\n","\n","metrics = []\n","for alt in df['alternation'].unique():\n","    alt_df = detailed_df[detailed_df['Alternation'] == alt]\n","    accuracy = accuracy_score(alt_df['True Label'], alt_df['Predicted Label'])\n","    f1_ent = f1_score(alt_df[alt_df['True Label'] == 1]['True Label'], alt_df[alt_df['True Label'] == 1]['Predicted Label'], pos_label=1) if len(alt_df[alt_df['True Label'] == 1]) > 0 else None\n","    f1_non_ent = f1_score(alt_df[alt_df['True Label'] == 0]['True Label'], alt_df[alt_df['True Label'] == 0]['Predicted Label'], pos_label=0) if len(alt_df[alt_df['True Label'] == 0]) > 0 else None\n","    metrics.append([alt, accuracy, f1_ent, f1_non_ent])\n","\n","metrics_df = pd.DataFrame(metrics, columns=['Alternation', 'Accuracy', 'F1 Entailment', 'F1 Non-Entailment'])\n","metrics_df.to_csv('metrics_by_alternation_DeBERTa_base.csv', index=False)\n","\n","overall_accuracy = accuracy_score(detailed_df['True Label'], detailed_df['Predicted Label'])\n","print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n"],"metadata":{"id":"F45EyLl8S1mh"},"execution_count":null,"outputs":[]}]}